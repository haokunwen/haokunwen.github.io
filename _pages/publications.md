---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

## Preprints
---

<details>
  <summary>FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval</summary>
  <pre><code>@article{li2025finecir,
  title={FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval},
  author={Li, Zixu and Fu, Zhiheng and Hu, Yupeng and Chen, Zhiwei and Wen, Haokun and Nie, Liqiang},
  journal={arXiv preprint arXiv:2503.21309},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, <b>Haokun Wen</b>, and Liqiang Nie.<br>
  <span class="pub-tag tag-preprint">ArXiv Preprint</span>
  <a href="https://arxiv.org/pdf/2503.21309" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>

<details>
  <summary>Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</summary>
  <pre><code>@article{chen2025dual,
  title={Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems},
  author={Chen, Xiaolin and Song, Xuemeng and Wen, Haokun and Guan, Weili and Zhao, Xiangyu and Nie, Liqiang},
  journal={arXiv preprint arXiv:2509.07817},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Xiaolin Chen, Xuemeng Song, <b>Haokun Wen</b>, Weili Guan, Xiangyu Zhao, and Liqiang Nie.<br>
  <span class="pub-tag tag-preprint">ArXiv Preprint</span>
  <a href="https://arxiv.org/abs/2509.07817v1" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>


## 2026
---

<details>
  <summary>D2MoRA: Diversity-Regulated Asymmetric MoE-LoRA Decomposition for Efficient Multi-Task Adaptation</summary>
  <pre><code> </code></pre>
</details>
<div class="pub-content">
  Jianhui Zuo, Xuemeng Song, <b>Haokun Wen</b>, Meng Liu, Yupeng Hu, Jiuru Wang, and Liqiang Nie. <br>
  <span class="pub-tag tag-conf">AAAI 2026</span>
</div>

## 2025
---

<details>
  <summary>Multi-modal Recommendation with Joint Content and Interaction Augmentation</summary>
  <pre><code></code></pre>
</details>
<div class="pub-content">
  Jiajie Deng, <b>Haokun Wen</b>, Xiao Han, Xuemeng Song, and Xiangyu Zhao.<br>
  <span class="pub-tag tag-conf">MMAsia 2025</span>
</div>

<details>
  <summary>Spatial Understanding from Videos: Structured Prompts Meet Simulation Data</summary>
  <pre><code>@inproceedings{zhang2025spatial,
  title={Spatial Understanding from Videos: Structured Prompts Meet Simulation Data},
  author={Zhang, Haoyu and Liu, Meng and Li, Zaijing and Wen, Haokun and Guan, Weili and Wang, Yaowei and Nie, Liqiang},
  booktitle={NeurIPS},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Haoyu Zhang, Meng Liu, Zaijing Li, <b>Haokun Wen</b>, Weili Guan, Yaowei Wang, and Liqiang Nie.<br>
  <span class="pub-tag tag-conf">NeurIPS 2025</span>
  <a href="https://arxiv.org/pdf/2506.03642" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>

<details>
  <summary>A Comprehensive Survey on Composed Image Retrieval</summary>
  <pre><code> </code></pre>
</details>
<div class="pub-content">
  Xuemeng Song, Haoqiang Lin, <b>Haokun Wen</b>, Bohan Hou, Mingzhu Xu, and Liqiang Nie.<br>
  <span class="pub-tag tag-journal">ACM TOIS 2025</span>
  <a href="https://arxiv.org/pdf/2502.18495" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>
    
<details>
  <summary>ENCODER: Entity Mining and Modification Relation Binding for Composed Image Retrieval</summary>
  <pre><code>@inproceedings{li2025encoder,
  title={ENCODER: Entity Mining and Modification Relation Binding for Composed Image Retrieval},
  author={Li, Zixu and Chen, Zhiwei and Wen, Haokun and Fu, Zhiheng and Hu, Yupeng and Guan, Weili},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Zixu Li, Zhiwei Chen, <b>Haokun Wen</b>, Zhiheng Fu, Yupeng Hu, and Weili Guan.<br>
  <span class="pub-tag tag-conf">AAAI 2025</span>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32541" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://sdu-l.github.io/ENCODER.github.io/" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-GitHub-181717"></a>
</div>

<details>
  <summary>FiRE: Enhancing MLLMs with Fine-Grained Context Learning for Complex Image Retrieval</summary>
  <pre><code>@inproceedings{hou2025fire,
  title={FiRE: Enhancing MLLMs with Fine-Grained Context Learning for Complex Image Retrieval},
  author={Hou, Bohan and Lin, Haoqiang and Song, Xuemeng and Wen, Haokun and Liu, Meng and Hu, Yupeng and Zhao, Xiangyu},
  booktitle={ACM SIGIR},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Bohan Hou, Haoqiang Lin, Xuemeng Song, <b>Haokun Wen</b>, Meng Liu, Yupeng Hu, and Xiangyu Zhao.<br>
  <span class="pub-tag tag-conf">ACM SIGIR 2025</span>
  <a href="https://dl.acm.org/doi/10.1145/3726302.3729979" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>

<details>
  <summary>Pseudo-triplet Guided Few-shot Composed Image Retrieval</summary>
  <pre><code>@inproceedings{hou2025pseudo,
  title={Pseudo-triplet Guided Few-shot Composed Image Retrieval},
  author={Hou, Bohan and Lin, Haoqiang and Wen, Haokun and Liu, Meng and Song, Xuemeng},
  booktitle={IJCNN},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Bohan Hou, Haoqiang Lin, <b>Haokun Wen</b>, Meng Liu, and Xuemeng Song.<br>
  <span class="pub-tag tag-conf">IJCNN 2025</span>
  <a href="https://arxiv.org/abs/2407.06001" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>  
   
<details>
  <summary>HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</summary>
  <pre><code>@inproceedings{chen2025hud,
  title={HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval},
  author={Chen, Zhiwei and Hu, Yupeng and Li, Zixu and Fu, Zhiheng and Wen, Haokun and Guan, Weili},
  booktitle={ACM Multimedia (ACM MM)},
  year={2025}
}</code></pre>
</details>
<div class="pub-content">
  Zhiwei Chen, Yupeng Hu, Zixu Li, Zhiheng Fu, <b>Haokun Wen</b>, and Weili Guan.<br>
  <span class="pub-tag tag-conf">ACM MM 2025</span>
  <a href="https://dl.acm.org/doi/10.1145/3746027.3755445" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>
     

## 2024
---

<details>
  <summary>Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval</summary>
  <pre><code>@inproceedings{wen2024simple,
  title={Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval},
  author={Wen, Haokun and Song, Xuemeng and Chen, Xiaolin and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
  booktitle={ACM SIGIR},
  year={2024}
}</code></pre>
</details>
<div class="pub-content">
  <b>Haokun Wen</b>, Xuemeng Song, Xiaolin Chen, Yinwei Wei, Liqiang Nie, and Tat-Seng Chua.<br>
  <span class="pub-tag tag-conf">ACM SIGIR 2024</span>
  <a href="https://arxiv.org/abs/2404.15875" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://github.com/haokunwen/DQU-CIR" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-GitHub-181717"></a>
  <a href="http://haokunwen.github.io/files/SIGIR24_DQU-CIR.pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Slides-PDF-0056b3"></a>
</div>

<details>
  <summary>Self-Training Boosted Multi-Factor Matching Network for Composed Image Retrieval</summary>
  <pre><code>@article{wen2024self,
  title={Self-Training Boosted Multi-Factor Matching Network for Composed Image Retrieval},
  author={Wen, Haokun and Song, Xuemeng and Yin, Jianhua and Wu, Jianlong and Guan, Weili and Nie, Liqiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year={2024}
}</code></pre>
</details>
<div class="pub-content">
  <b>Haokun Wen</b>, Xuemeng Song, Jianhua Yin, Jianlong Wu, Weili Guan, and Liqiang Nie.<br>
  <span class="pub-tag tag-journal">IEEE TPAMI 2024</span>
  <a href="https://ieeexplore.ieee.org/abstract/document/10373096" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://anosite.wixsite.com/limn" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-Project-181717"></a>
</div>

<details>
  <summary>Fine-Grained Textual Inversion Network for Zero-Shot Composed Image Retrieval</summary>
  <pre><code>@inproceedings{lin2024fine,
  title={Fine-Grained Textual Inversion Network for Zero-Shot Composed Image Retrieval},
  author={Lin, Haoqiang and Wen, Haokun and Song, Xuemeng and Liu, Meng and Hu, Yupeng and Nie, Liqiang},
  booktitle={ACM SIGIR},
  year={2024}
}</code></pre>
</details>
<div class="pub-content">
  Haoqiang Lin, <b>Haokun Wen</b>, Xuemeng Song, Meng Liu, Yupeng Hu, and Liqiang Nie.<br>
  <span class="pub-tag tag-conf">ACM SIGIR 2024</span>
  <a href="https://dl.acm.org/doi/10.1145/3626772.3657831" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://github.com/ZiChao111/FTI4CIR" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-GitHub-181717"></a>
</div>

<details>
  <summary>Differential-Perceptive and Retrieval-Augmented MLLM for Change Captioning</summary>
  <pre><code>@inproceedings{zhang2024differential,
  title={Differential-Perceptive and Retrieval-Augmented MLLM for Change Captioning},
  author={Zhang, Xian and Wen, Haokun and Wu, Jianlong and Qin, Pengda and Xue, Hui and Nie, Liqiang},
  booktitle={ACM Multimedia (ACM MM)},
  year={2024}
}</code></pre>
</details>
<div class="pub-content">
  Xian Zhang, <b>Haokun Wen</b>, Jianlong Wu, Pengda Qin, Hui Xue, and Liqiang Nie.<br>
  <span class="pub-tag tag-conf">ACM MM 2024</span>
  <a href="https://openreview.net/attachment?id=eiGs5VCsYM&name=pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://github.com/xianzhangzx/FINER-MLLM" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-GitHub-181717"></a>
</div>

<details>
  <summary>Interactive Garment Recommendation with User in the Loop</summary>
  <pre><code>@article{becattini2024interactive,
  title={Interactive Garment Recommendation with User in the Loop},
  author={Becattini, Federico and Chen, Xiaolin and Puccia, Andrea and Wen, Haokun and Song, Xuemeng and Nie, Liqiang and Del Bimbo, Alberto},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  year={2024}
}</code></pre>
</details>
<div class="pub-content">
  Federico Becattini, Xiaolin Chen, Andrea Puccia, <b>Haokun Wen</b>, Xuemeng Song, Liqiang Nie, and Alberto Del Bimbo.<br>
  <span class="pub-tag tag-journal">ACM ToMM 2024</span>
  <a href="https://arxiv.org/abs/2402.11627" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>

## 2023
---
<details>
  <summary>Target-Guided Composed Image Retrieval</summary>
  <pre><code>@inproceedings{wen2023target,
  title={Target-Guided Composed Image Retrieval},
  author={Wen, Haokun and Zhang, Xian and Song, Xuemeng and Wei, Yinwei and Nie, Liqiang},
  booktitle={ACM Multimedia (ACM MM)},
  year={2023}
}</code></pre>
</details>
<div class="pub-content">
  <b>Haokun Wen</b>, Xian Zhang, Xuemeng Song, Yinwei Wei, and Liqiang Nie.<br>
  <span class="pub-tag tag-conf">ACM MM 2023</span>
  <a href="https://arxiv.org/pdf/2309.01366.pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://anosite.wixsite.com/tg-cir" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-Project-181717"></a>
  <a href="http://haokunwen.github.io/files/mm23-TG-CIR.pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Slides-PDF-0056b3"></a>
</div>

<details>
  <summary>Finetuning Language Models for Multimodal Question Answering</summary>
  <pre><code>@inproceedings{zhang2023finetuning,
  title={Finetuning Language Models for Multimodal Question Answering},
  author={Zhang, Xin and Xie, Wen and Dai, Ziqi and Rao, Jun and Wen, Haokun and Luo, Xuan and Zhang, Meishan and Zhang, Min},
  booktitle={ACM Multimedia (ACM MM)},
  year={2023}
}</code></pre>
</details>
<div class="pub-content">
  Xin Zhang, Wen Xie, Ziqi Dai, Jun Rao, <b>Haokun Wen</b>, Xuan Luo, Meishan Zhang, and Min Zhang.<br>
  <span class="pub-tag tag-conf">ACM MM 2023 (Grand Challenge)</span> <span class="pub-tag tag-award">üèÜ Ranked 1st in VTQA</span>
  <a href="http://haokunwen.github.io/files/acmmm2023_grandchallenge.pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
</div>

<details>
  <summary>Egocentric Early Action Prediction via Multimodal Transformer-Based Dual Action Prediction</summary>
  <pre><code>@article{guan2023egocentric,
  title={Egocentric Early Action Prediction via Multimodal Transformer-Based Dual Action Prediction},
  author={Guan, Weili and Song, Xuemeng and Wang, Kejie and Wen, Haokun and Ni, Hongda and Wang, Yaowei and Chang, Xiaojun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)},
  year={2023}
}</code></pre>
</details>
<div class="pub-content">
  Weili Guan, Xuemeng Song, Kejie Wang, <b>Haokun Wen</b>, Hongda Ni, Yaowei Wang, and Xiaojun Chang.<br>
  <span class="pub-tag tag-journal">IEEE TCSVT 2023</span>
  <a href="http://haokunwen.github.io/files/tcsvt2023.pdf" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Paper-PDF-b31b1b"></a>
  <a href="https://trace729.wixsite.com/trace" style="margin-left: 2px;"><img src="https://img.shields.io/badge/Code-Project-181717"></a>
</div>

  - **Egocentric Early Action Prediction via Multimodal Transformer-Based Dual Action Prediction**    
    Weili Guan, Xuemeng Song, Kejie Wang, **Haokun Wen**, Hongda Ni, Yaowei Wang, and Xiaojun Chang.   
    IEEE TCSVT, 2023. [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/tcsvt2023.pdf) [[Code]](https://trace729.wixsite.com/trace) [[BibTex]](https://dblp.org/rec/journals/tcsv/GuanSWWNWC23.html?view=bibtex)    

**2022**
  - **Personalized Fashion Compatibility Modeling via Metapath-guided Heterogeneous Graph Learning**  
    Weili Guan, Fangkai Jiao, Xuemeng Song, **Haokun Wen**, Chung-Hsing Yeh, and Xiaojun Chang.    
    In ACM SIGIR 2022 (full paper). [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/acmsigir2022.pdf) [[Code]](https://anosite.wixsite.com/pfcm) [[BibTex]](https://dblp.org/rec/conf/sigir/GuanJSWYC22.html?view=bibtex)     

  - **Partially Supervised Compatibility Modeling**  
    Weili Guan, **Haokun Wen**, Xuemeng Song, Chun Wang, Chung-Hsing Yeh, Xiaojun Chang, and Liqiang Nie.  
    IEEE TIP, 2022. [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/tip2022.pdf) [[Code]](https://site2750.wixsite.com/ps-ocm) [[BibTex]](https://dblp.org/rec/journals/tip/GuanWSWYCN22.html?view=bibtex)  

**2021**
  - **Comprehensive Linguistic-Visual Composition Network for Image Retrieval**  
    **Haokun Wen**, Xuemeng Song, Xin Yang, Yibing Zhan, and Liqiang Nie.  
    In ACM SIGIR 2021 (full paper). [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/acmsigir2021.pdf) [[Code]](https://site2750.wixsite.com/clvcnet) [[BibTex]](https://dblp.org/rec/conf/sigir/WenSYZN21.html?view=bibtex)  

  - **Multimodal Compatibility Modeling via Exploring the Consistent and Complementary Correlations**  
    Weili Guan, **Haokun Wen**, Xuemeng Song, Chung-Hsing Yeh, Xiaojun Chang, and Liqiang Nie.  
    In ACM MM 2021 (full paper). [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/acmmm2021.pdf) [[Code]](https://site2750.wixsite.com/mmocm) [[BibTex]](https://dblp.org/rec/conf/mm/GuanWSYCN21.html?view=bibtex)  

  - **Attribute-wise Explainable Fashion Compatibility Modeling**  
    Xin Yang, Xuemeng Song, Fuli Feng, **Haokun Wen**, Ling-Yu Duan, and Liqiang Nie.  
    ACM ToMM, 2021. [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/acmtomm2021.pdf) [[Code]](https://joeyangbuer.wixsite.com/exfcm) [[BibTex]](https://dblp.org/rec/journals/tomccap/YangSFWDN21.html?view=bibtex)    

**2020**
  - **Generative Attribute Manipulation Scheme for Flexible Fashion Search**  
    Xin Yang, Xuemeng Song, Xianjing Han, **Haokun Wen**, Jie Nie, and Liqiang Nie.  
    In ACM SIGIR 2020 (full paper). [![Paper](https://img.shields.io/badge/Paper-PDF-b31b1b)](http://haokunwen.github.io/files/acmsigir2020.pdf) [[Code]](https://joeyangbuer.wixsite.com/amgan) [[BibTex]](https://dblp.org/rec/conf/sigir/YangSHWNN20.html?view=bibtex)  
    




